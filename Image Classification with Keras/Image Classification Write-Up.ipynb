{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea59ac19-1bdd-4bea-901c-aaff66a4f126",
   "metadata": {},
   "source": [
    "# PS7 Written Portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca9a2",
   "metadata": {},
   "source": [
    "#### 1. (25pt) Reasonably well written code that works. You should get the code to work, and explain what the more complex parts are doing in comments.\n",
    "#### You may keep code as a notebook, or as a separate code file.\n",
    "#### If your code includes very complicated parts, you may add explanations of those in your text too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d675e",
   "metadata": {},
   "source": [
    "The code file is submitted on Canvas, comments are made, and the code is functioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf7b06-9b7c-4eda-9dad-05921d63ae4e",
   "metadata": {},
   "source": [
    "#### 2. (75pt) Written explanations/results, either in notebook, or as a separate text file.\n",
    "\n",
    "#### (a) (25pt) Do your “best” to get good results. Ideal results distinguish all these results 100% perfectly, work fast and contain only ’few' parameters. But this is probably an unachievable goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504bda0-ce26-4a3e-bf13-2f80af28320d",
   "metadata": {},
   "source": [
    "We tried a variety of models to achieve the best possible accuracy. Some of the models tried are listed in the comment part of the code file. Details on what we changed in the code is outlined below in question 2b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ceae4-5ecb-45e2-bfd9-2d510186713a",
   "metadata": {},
   "source": [
    "#### (b) (30pt) Explain what did you try and what did you get.\n",
    "    \n",
    "   - List here different models, layer structures, image pre-processing that you tried.\n",
    "   \n",
    "   - Explain it in a readable form, either in the notebook text cells, or as a separate written document.\n",
    "   \n",
    "   - Explain also the technical side. Was your computer good enough? Did you use some other kind of computing resources? Did you have to scale down the dataset? Were you able to run enough epochs?\n",
    "\n",
    "Note: point 2a is about making your best reasonable effort. This point is about explaining\n",
    "these efforts well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26ed88-0355-46bf-8812-502abb59b1d0",
   "metadata": {},
   "source": [
    "The first thing we did was add the training and validation data to the Google Drive and adapted the squares-circles code to run this data. We got a pretty low validation accuracy that was around 25-30%. \n",
    "\n",
    "Next, We filtered the training and validation images to include only 2 languages, EN and ZN. We kept the number of epochs at 5 while testing so the code took less time to run as We changed different variables. Increasing the number of nodes from 2 to 5 helped improve accuracy, so we kept 5 nodes throughout the testing process. We messed with the image target size but found that increasing it didn't help much so We kept it at a width and height of 25. With 5 epochs and (25, 25) image target size, we were at an accuracy that was 95% or higher for 2 languages. Then, we added a third language, \"TH\". With the addition of the third language and the same code We used for 2 languages, we were at an accuracy of about 70%. \n",
    "\n",
    "We decided to go back to using all 5 languages at this point and see where the accuracy was at. We got accuracies ranging from 50-60% after running the code several times. We tried to sample only 10,000 data from the dataset to improve the speed of testing. We changed the number of epochs back to 10 and changed the image target width at 50 and height at 50. This gave us a lower accuracy of about 40% and was extremely slow to run. We then changed the epochs back to 5 and once again set the image target width and height to 25. \n",
    "\n",
    "Throughout the testing process with 5 languages, we messed with the convolutional layers by commenting different ones out each time we ran the code. Keeping all 3 convolutional layers and the final dense layer from squares-circles worked best so we kept that. We also messed with the kernel size, and settled on setting it to 3 as that gave me the best accuracy.\n",
    "\n",
    "Towards the end, We started using a sample of 5000 training images rather than the full set to speed up the testing process. We ran the code a few times with different amounts of epochs - 5, 10, and 20. We noticed that the accuracy was much higher when using the full data set rather than a sample, so we used all training images for the final code.\n",
    "\n",
    "Overall, we noticed that accuracy swings back and forth quite a bit with all 5 languages. In the final code, we used 10 epochs. Despite running the code numerous times, we struggled to get the accuracy to surpass around 75% for all of the languages. There were several points when we used 20 epochs but having 20 rather than 10 didn't always help in accuracy. We reran the final code several times and the validation accuracy fluctuated quite a bit, giving me accuracies ranging from 55-75%.\n",
    "\n",
    "One computer worked fine but was slow to process the code. Thus, we decided to run the code and experiment with different parameters with two computers to speed up the process. Both computers had strong enough CPU and GPU to run the code. We ran the code through Google CoLab and pulled the training and validation data from Google Drive. We were able to run enough epochs but it did take a very long time to do so unless we used a sample of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137ffe2-ae08-48ec-9e06-144c4b865dbb",
   "metadata": {},
   "source": [
    "#### (c) (20pt) Present your “final results”, your model that you consider the best (this should be the code your submit), and its main hyperparameters and performance indicators. How many epochs did you use? How long time did it take? Show your confusion matrix and compute accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dedbfb-e77a-477e-ad82-b34a60e81ce6",
   "metadata": {},
   "source": [
    "The best performed model for us is presented in the code file. The best performed model ended up using a image size of 25*25, 10 epochs, 3 convolutional layers with kernal size of 3, and 2 dense layers. The final time we ran the code on all 5 languages, we got a validation accuracy of 75.74% and training accuracy was about 76.49%. We used 5 epochs and it took about 40 seconds per epoch or 400 seconds total to run. \n",
    "\n",
    "The confusion matrix for the validation data is presented below:\n",
    "\n",
    "| Predicted | DA  | EN   | RU   | TH   | ZN  |\n",
    "|-----------|-----|------|------|------|-----|\n",
    "| Category  |     |      |      |      |     |\n",
    "| DA        | 193 | 352  | 90   | 340  | 44  |\n",
    "| EN        | 85  | 1921 | 2    | 39   | 3   |\n",
    "| RU        | 57  | 197  | 1126 | 291  | 22  |\n",
    "| TH        | 3   | 24   | 57   | 1802 | 13  |\n",
    "| ZN        | 25  | 44   | 50   | 195  | 992 |\n",
    "\n",
    "The confusion matrix for the training data is presented below:\n",
    "\n",
    "| Predicted | DA  | EN   | RU   | TH   | ZN  |\n",
    "|-----------|-----|------|------|------|-----|\n",
    "| Category  |     |      |      |      |     |\n",
    "| DA        | 855 | 1394  | 398   | 1257  | 191  |\n",
    "| EN        | 377  | 7901 | 21    | 131   | 12   |\n",
    "| RU        | 197  | 689  | 4428 | 1097  | 100  |\n",
    "| TH        | 30   | 94   | 227   | 7023 | 51  |\n",
    "| ZN        | 76  | 197   | 205   | 748  | 4170 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
