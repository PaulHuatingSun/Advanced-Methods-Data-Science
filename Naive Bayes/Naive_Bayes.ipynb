{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c79e86d-f58f-46d0-b280-0c7a218a3245",
   "metadata": {
    "id": "3c79e86d-f58f-46d0-b280-0c7a218a3245",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5b789-282a-457f-8fb8-6af103d45007",
   "metadata": {
    "id": "9ac5b789-282a-457f-8fb8-6af103d45007"
   },
   "source": [
    "# Load and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce8d9c-17c0-4b77-ad02-f8b21e687412",
   "metadata": {
    "id": "fbce8d9c-17c0-4b77-ad02-f8b21e687412"
   },
   "source": [
    "    1. (1pt) Load and clean data. Feel free to copy-paste from your PS05 solution.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0cf0c4-29ee-44ca-9675-0546888b61f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "0f0cf0c4-29ee-44ca-9675-0546888b61f7",
    "outputId": "77644a54-ca5c-4d0a-d01c-f2833ace81a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2893 entries, 0 to 2892\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   spam     2893 non-null   bool  \n",
      " 1   files    2893 non-null   object\n",
      " 2   message  2893 non-null   object\n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 70.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>files</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>False</td>\n",
       "      <td>9-671msg1.txt</td>\n",
       "      <td>Subject: sociolinguistics  mediated discourse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>False</td>\n",
       "      <td>6-266msg3.txt</td>\n",
       "      <td>Subject: bisfai deadline extension !  bisfai d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>False</td>\n",
       "      <td>5-1430msg1.txt</td>\n",
       "      <td>Subject: query : \" grasshopper mind \" ?  are y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>True</td>\n",
       "      <td>spmsgb164.txt</td>\n",
       "      <td>Subject: university degree programs  universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>True</td>\n",
       "      <td>spmsgc117.txt</td>\n",
       "      <td>Subject: info on office 2000 pro  microsoft of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam           files                                            message\n",
       "349   False   9-671msg1.txt  Subject: sociolinguistics  mediated discourse ...\n",
       "1172  False   6-266msg3.txt  Subject: bisfai deadline extension !  bisfai d...\n",
       "695   False  5-1430msg1.txt  Subject: query : \" grasshopper mind \" ?  are y...\n",
       "1735   True   spmsgb164.txt  Subject: university degree programs  universit...\n",
       "2574   True   spmsgc117.txt  Subject: info on office 2000 pro  microsoft of..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "email_df = pd.read_csv('/home/jovyan/INFO371PS/Data/lingspam-emails.csv', sep = \"\\t\")\n",
    "# Drop the NA values and print basic information\n",
    "email_df = email_df.dropna(subset=['spam', 'message'])\n",
    "email_df.info()\n",
    "# Browse a handful of emails\n",
    "email_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e31ee-f1ab-437b-b1aa-e212ad7ebf21",
   "metadata": {
    "id": "ec1e31ee-f1ab-437b-b1aa-e212ad7ebf21"
   },
   "source": [
    "    2. (2pt) Vectorize emails so you have a DTM (I’ll refer to this as the design matrix X) and the spam/non-spam indicator y. If you don’t know how to do it, you can just use the code below:\n",
    "    How many different documents (emails) and different tokens (words) do you have in these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb99d989-ee95-48cb-9621-cd769b4cd712",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "cb99d989-ee95-48cb-9621-cd769b4cd712",
    "outputId": "f372ec93-c4c4-4879-e5f9-753f1cc89a18"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "# define vectorizer\n",
    "X = vectorizer.fit_transform(email_df.message)\n",
    "# vectorize your data. Note: this creates a sparse matrix,\n",
    "# use .toarray() if you run into trouble\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "# in case you want to see what are the actual wordsvvv\n",
    "y = email_df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67e12e0-3099-4032-849c-567f0796bf90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a67e12e0-3099-4032-849c-567f0796bf90",
    "outputId": "e568a190-105a-483b-c853-a5fba760dcb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2893 emails in the dataset.\n",
      "There are 60925 different tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", email_df.shape[0], \"emails in the dataset.\")\n",
    "print(\"There are\", len(vocabulary), \"different tokens in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb58494-b72e-4b2c-adfb-909211e53620",
   "metadata": {
    "id": "7fb58494-b72e-4b2c-adfb-909211e53620"
   },
   "source": [
    "    3. (2pt) Split data into training/validation chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44bf81f-0f52-40ea-992e-beb39d5c54e5",
   "metadata": {
    "id": "d44bf81f-0f52-40ea-992e-beb39d5c54e5"
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b0951-81d0-47d3-bd0f-8e57ee4104d0",
   "metadata": {
    "id": "1a0b0951-81d0-47d3-bd0f-8e57ee4104d0"
   },
   "source": [
    "    4. (2pt) Design a scheme to name your variables so you can understand (and you grader can understand too!) which mathematical concept it refers to. You need variables like (see Lecture notes, Ch 7.3 for more examples/explanations):\n",
    "    •Pr(S=1): probability of spam\n",
    "    •Pr(S=0|W=1): probability the email is not spam given it cointains the word W\n",
    "    •log Pr(W=1): log probability of word present\n",
    "    •ℓ(S=1|W): log-likelihood of email being spam, given vector of words it contains.\n",
    "    Explain how do you name these examples values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a88329-976f-4586-a3d5-48662f267d2f",
   "metadata": {
    "id": "37a88329-976f-4586-a3d5-48662f267d2f"
   },
   "source": [
    "| Variable Name | Notation                 | Definition                                                          |\n",
    "|---------------|--------------------------|--------------------------------------------------------------------|\n",
    "| Pr_S1         | Pr(S = 1)           | Probability that the email is spam                               |\n",
    "| Pr_S0         | Pr(S = 0)           | Probability that the email is non-spam                           |\n",
    "| Pr_W1_S1      | Pr(W = 1\\|S = 1)     | Probability of the word is present given the condition that the email is spam|\n",
    "| Pr_W1_S0      | Pr(W = 1\\|S = 0)     | Probability of the word is present given the condition that the email is non-spam |\n",
    "| logPr_S1      | log Pr(S = 1)       | Log probability that the email is spam                            |\n",
    "| logPr_S0      | log Pr(S = 0)       | Log probability that the email is non-spam                        |\n",
    "| logPr_W1_S1   | log Pr(W = 1\\|S = 1) | Log probability that the word is present in spam emails             |\n",
    "| logPr_W1_S0   | log Pr(W = 1\\|S = 0) | Log probability that the word is present in non-spam emails         |\n",
    "| logPr_S1_W1   | log Pr(W = 1\\|S = 1) | Log probability that the email is spam given the condition that the word is present |\n",
    "| logPr_S0_W1   | log Pr(W = 1\\|S = 0) | Log probability that the email is non-spam given the condition that the word is present |\n",
    "| L_S1          | Pr(S = 1\\|W)         | Log-likelihood for spam                                            |\n",
    "| L_S0          | Pr(S = 0\\|W)         | Log-likelihood for non-spam                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a7ebf-eff0-47e1-9f55-0687fdc9a15a",
   "metadata": {
    "id": "cf2a7ebf-eff0-47e1-9f55-0687fdc9a15a"
   },
   "source": [
    "# Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e1a9c-0394-4043-aafd-27d72c745fa6",
   "metadata": {
    "id": "881e1a9c-0394-4043-aafd-27d72c745fa6"
   },
   "source": [
    "    1. (4pt) Here is a small excerpt from the initial DTM (before you split it into training/validation), corresponding to rows 983 to 985, and to columns 40,042–40,046:\n",
    "    What do these numbers show:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d742b-3044-4262-80e3-b81e7688ffc5",
   "metadata": {
    "id": "722d742b-3044-4262-80e3-b81e7688ffc5"
   },
   "source": [
    "    (a) which emails do the rows correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2947680-d711-448f-9b4d-c3002261e239",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2947680-d711-448f-9b4d-c3002261e239",
    "outputId": "aea9a406-635e-44d3-8ec6-394ede0abe24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rows corresponds to the emails at indices 983 to 985:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "982    Subject: summary : parsing of ambiguous sequen...\n",
       "983    Subject: re : sapir - whorf and what to tell s...\n",
       "984    Subject: call for contributions  call for cont...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the messages corresponds to rows 983 to 985\n",
    "print(\"The rows corresponds to the emails at indices 983 to 985:\")\n",
    "email_df.message[982:985]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac632132-bba4-4749-903b-a6a450792a67",
   "metadata": {
    "id": "ac632132-bba4-4749-903b-a6a450792a67"
   },
   "source": [
    "    (b) Which words do the columns correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ac1800-e2d5-4170-a95d-189635b48bc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63ac1800-e2d5-4170-a95d-189635b48bc2",
    "outputId": "ea05bc62-ede3-4bee-d2c9-ee6c598628ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nooteboom', 'nootka', 'nope', 'nor', 'nora'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the corresponding words to the columns 40,042 to 40,046\n",
    "vocabulary[40041:40046]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348dfc3b-6b54-488a-be8e-c3dd455b83d5",
   "metadata": {
    "id": "348dfc3b-6b54-488a-be8e-c3dd455b83d5"
   },
   "source": [
    "The words the columns correspond to are: \"nootka\", \"nope\", \"nor\", \"nora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bb471-f657-4f6c-9ce4-c73ea8f5aa07",
   "metadata": {
    "id": "e40bb471-f657-4f6c-9ce4-c73ea8f5aa07"
   },
   "source": [
    "    (c) What do the “1”-s in the middle of the table mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b52367b-d446-446b-9109-c2653e3ae5f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b52367b-d446-446b-9109-c2653e3ae5f7",
    "outputId": "3ff44ae1-0168-4dcd-e349-8f14401e41fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the corresponding rows and columns\n",
    "X[982:985, 40041:40046].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d23151-321d-410a-acfe-8125eac5b95d",
   "metadata": {
    "id": "78d23151-321d-410a-acfe-8125eac5b95d"
   },
   "source": [
    "The first 1 in the table indicates that the presence of word \"nootka\" in the email at row index 984. The second 1 in the table indicates the presence of the word \"nor\" in the email at row index 984."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831be99-8004-4140-91e5-ae6f3185460d",
   "metadata": {
    "id": "2831be99-8004-4140-91e5-ae6f3185460d"
   },
   "source": [
    "    (d) What do the zeros mean?\n",
    "    Note: you should have exactly the same numbers in your analysis, this is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fYiGzcpF5M2g",
   "metadata": {
    "id": "fYiGzcpF5M2g"
   },
   "source": [
    "The zeros mean that the corresponding word does not appear in the correspoinding email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4661c99-1204-4c31-857a-c96a5a6bc189",
   "metadata": {
    "id": "e4661c99-1204-4c31-857a-c96a5a6bc189"
   },
   "source": [
    "    2. (2pt) What is the accuracy of the naive model that predicts all emails into the majority category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5f314a-4ab0-4a4f-9552-974f9b4f7a71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b5f314a-4ab0-4a4f-9552-974f9b4f7a71",
    "outputId": "19f06b41-7aa3-4885-c391-39a00cd60f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8337366055997235\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the naive model that predicts all emails into the majority category\n",
    "count_S0 = np.size(y) - np.count_nonzero(y)\n",
    "count_S1 = np.count_nonzero(y)\n",
    "\n",
    "if count_S1 > count_S0:\n",
    "  print(np.mean(y))\n",
    "else:\n",
    "  print(1 - np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wPN7mczN7e1r",
   "metadata": {
    "id": "wPN7mczN7e1r"
   },
   "source": [
    "The accuracy of the naive model that predicts all emails into the majority category (non-spam emails) is about 83.37%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca35af-fdd1-47db-bf2c-c5c3cdc57e4c",
   "metadata": {
    "id": "49ca35af-fdd1-47db-bf2c-c5c3cdc57e4c"
   },
   "source": [
    "    3. (3pt) Compute the unconditional (log) probability that the email is spam/non-spam, log Pr(S=1), and log Pr(S=0). These probabilities are based on the values of y(i.e. spam) alone. They do not contain information about the words in emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9c07d5-3d86-4fc2-9c86-6e6fdf14d2de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa9c07d5-3d86-4fc2-9c86-6e6fdf14d2de",
    "outputId": "47363e3d-b312-4de1-c18c-56b1bf4eeecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability of the email being spam is: -1.790895538288791\n",
      "Log Probability of the email being non-spam is: -0.1824944325831309\n"
     ]
    }
   ],
   "source": [
    "# Compute the log probabilities for spam/non-spam emails\n",
    "Pr_S1 = np.mean(y_train == 1)\n",
    "# Pr_S0 = np.mean(y_train == 0)\n",
    "Pr_S0 = 1 - Pr_S1\n",
    "logPr_S1 = np.log(Pr_S1)\n",
    "logPr_S0 = np.log(Pr_S0)\n",
    "print('Log Probability of the email being spam is:', logPr_S1)\n",
    "print('Log Probability of the email being non-spam is:', logPr_S0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43457215-4ca9-4dff-936d-b71ccf2d7632",
   "metadata": {
    "id": "43457215-4ca9-4dff-936d-b71ccf2d7632"
   },
   "source": [
    "    4. (8pt) For each word w, compute the (log) probability that the word is present in spam emails, log Pr(W=1|S=1), and (log) probability that the word is present in non-spam emails, log Pr(W= 1|S=0). These probabilities can easily be calculated from counts of how many times these words are present for each class.\n",
    "    Hint: these computations are based on your BOW-s X. Look at ways to sum along columns in this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ceffaee-db36-489e-a2ce-0571f1d734f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ceffaee-db36-489e-a2ce-0571f1d734f8",
    "outputId": "a4f0691d-adbd-4f1d-e7ab-e0abe7c34ccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-1.12752363, -1.12752363, -5.95583737, ...,        -inf,\n",
       "          -5.95583737,        -inf]]),\n",
       " matrix([[-1.8308972 , -2.94911796, -7.56423848, ..., -7.56423848,\n",
       "                 -inf,        -inf]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the Pr(W=1|S=1) and Pr(W=1|S=0)\n",
    "Pr_W1_S1 = np.mean(X_train[y_train == 1], axis = 0)\n",
    "Pr_W1_S0 = np.mean(X_train[y_train == 0], axis = 0)\n",
    "# Compute the log of Pr(W=1|S=1) and Pr(W=1|S=0)\n",
    "logPr_W1_S1 = np.log(Pr_W1_S1)\n",
    "logPr_W1_S0 = np.log(Pr_W1_S0)\n",
    "# Print following results\n",
    "logPr_W1_S1, logPr_W1_S0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d00a8f-4a4e-4d7e-ad1f-6d2a032a7973",
   "metadata": {
    "id": "e9d00a8f-4a4e-4d7e-ad1f-6d2a032a7973"
   },
   "source": [
    "    5. (1pt) What should be the dimension of your log Pr(W=1|S=0)and log Pr(W=1|S=0)vectors?\n",
    "    Explain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa2d644-e422-44c3-ab82-88128e8303e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60925)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the log Pr(W=1|S=0)\n",
    "logPr_W1_S0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce52972-1caa-48cf-98f3-e850fd0aafb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60925)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the log Pr(W=1|S=1)\n",
    "logPr_W1_S1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e49af-a359-43c8-abd7-cfef45bc7135",
   "metadata": {},
   "source": [
    "The dimension should be 1 row with 60925 columns. The 1 row holds the probabilities for each of the 60925 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f407e-506b-483f-92aa-fb6d18eeea69",
   "metadata": {
    "id": "4a3f407e-506b-483f-92aa-fb6d18eeea69"
   },
   "source": [
    "    6. (10pt) For both classes, S=1 and S=0, compute the log-likelihood that the email belongs to this class. Log-likelihood is given as (7.3.20 and 7.3.21, page 270 for now) in lecture notes, and the equations in Schutt “Doing Data Science”, page 102. \n",
    "    Computing the likelihoods involves sums of the previously computed probabilities, log Pr(W=1|S), and BOW elements xij. Start by doing this by whatever way you can get it done (e.g. loops). The most important thing is that you understand what you do!\n",
    "    But if you want to write efficient code, use matrix product instead (it is ∼1000×faster than loops). See Lecture Notes (7.30.30) for how to do it with matrix product. You can also check out np.apply_along_axis as an alternative way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a234624-24f1-4f67-8d35-5cac88d03262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a234624-24f1-4f67-8d35-5cac88d03262",
    "outputId": "942ed097-5796-4620-dc91-791a6d54c0ca"
   },
   "outputs": [],
   "source": [
    "# Compute log Pr(W=1|S=1) and log Pr(W=1|S=0)\n",
    "L_S1 = X_val @ logPr_W1_S1.T + logPr_S1\n",
    "L_S0 = X_val @ logPr_W1_S0.T + logPr_S0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "-GOnnn8G1gBd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GOnnn8G1gBd",
    "outputId": "e6a46136-79db-4e66-faa0-cd03fe7648b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-1.12752363, -1.12752363, -5.95583737, ...,        -inf,\n",
       "         -5.95583737,        -inf]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logPr_W1_S1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfdebe-12d9-444b-b155-0e07ecfbf7aa",
   "metadata": {
    "id": "6ebfdebe-12d9-444b-b155-0e07ecfbf7aa"
   },
   "source": [
    "    7. (2pt) How many log-likelihoods you have to compute? Explain why do you have to have this many log-likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b261eadf-012a-4cd1-9ae9-920d110a513f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b261eadf-012a-4cd1-9ae9-920d110a513f",
    "outputId": "6044713e-a169-443b-f55c-0fb1901bf32e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the total number of log-likelihoods \n",
    "L_S1.shape[0] + L_S0.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "la-49tfPCmFw",
   "metadata": {
    "id": "la-49tfPCmFw"
   },
   "source": [
    "1158 log-likelihoods were computed. This is because there are 579 emails in the validation data and 2 log-likelihoods for each emails to make predictions. 579 * 2 = 1158."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8f199-9504-41be-9dc8-5ef1bc305a3f",
   "metadata": {
    "id": "a6b8f199-9504-41be-9dc8-5ef1bc305a3f"
   },
   "source": [
    "    8. (7pt) Based on the log-likelihoods, predict the class S=1 or S=0 for each email in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d588bab1-30ed-4ea8-b885-0ba31c496f7d",
   "metadata": {
    "id": "d588bab1-30ed-4ea8-b885-0ba31c496f7d"
   },
   "outputs": [],
   "source": [
    "# Make predictions based on the log-likelihoods and make them 1/0\n",
    "prediction = L_S1 > L_S0\n",
    "prediction = prediction * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b46c56-fee3-4b12-9b41-d690427e3f67",
   "metadata": {
    "id": "f8b46c56-fee3-4b12-9b41-d690427e3f67"
   },
   "source": [
    "    9. (3pt) Print the resulting confusion matrix and accuracy (feel free to use existing libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6acc3ee6-807d-46cf-97b9-1bda7bd1fd22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6acc3ee6-807d-46cf-97b9-1bda7bd1fd22",
    "outputId": "c1e44ad5-176c-43d3-f3bc-bc2bf73e1fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[484   0]\n",
      " [ 81  14]]\n",
      "Accuracy:\n",
      "0.8601036269430051\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, prediction))\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y_val, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141906d-c02e-401d-9969-40caea8d14a4",
   "metadata": {
    "id": "1141906d-c02e-401d-9969-40caea8d14a4"
   },
   "source": [
    "    10. (5pt) If your results are like mine, you can see that the results are not impressive at all, your model works no better than the naive guess. Explain why do you get such mediocre results.\n",
    "    Hint: this is related to infinites, where are those coming from, and why they make the model useless? See also the smoothing-related discussion in Lecture Notes at the end of the Naive Bayes (Section 7.3.3), before Example 7.3.\n",
    "    Note: just explain, but do not do anything about it! We’ll attack the problem in the next question with smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6aa39-4c16-460a-afcf-d7652a85e9d3",
   "metadata": {
    "id": "2bc3bdc4-1260-44c4-b674-4b083c1d915b"
   },
   "source": [
    "The not impressive results are due to the number of -infinity values in the log Pr(W = 1|S = 0) and log Pr(W = 1|S = 1). The -infinity values come from using log and because these log values are so small, we cannot determine whether Pr(W = 1|S = 0) or Pr(W = 1|S = 1) is larger to make an accurate prediction on the emails.\n",
    "\n",
    "Rare words that only occur in one class (spam or non spam) are categorized as having a 100% probability for the class they belong in and 0% probability for the one they're not in. Using log-likelihood only shifts the issue from zero-likelihood to minus-infinity-log-likelihood, which is no help. This makes the model useless because the other words in the text play no role - zero/negative infinity remains zero/negative infinity even when multiplied by other positive probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d25b5-4193-4976-b5ae-80f5cecc07eb",
   "metadata": {
    "id": "281d25b5-4193-4976-b5ae-80f5cecc07eb"
   },
   "source": [
    "# Add smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4854b2-8add-4923-9e2a-b294bb5d627a",
   "metadata": {
    "id": "6c4854b2-8add-4923-9e2a-b294bb5d627a"
   },
   "source": [
    "    1. (2pt) As you will be doing validation below, your first task is to mold what you did above into two functions: one for fitting and another one for predicting. You can mostly copy-paste your code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3915611a-71f8-4445-b5f9-46a4600ff286",
   "metadata": {
    "id": "3915611a-71f8-4445-b5f9-46a4600ff286"
   },
   "outputs": [],
   "source": [
    "# Create a function that add smoothing to the model\n",
    "def fitting(X_train, y_train, alpha):\n",
    "    total_spam = (y_train == 1).sum()\n",
    "    total_non_spam = (y_train == 0).sum()\n",
    "    \n",
    "    logPr_S1 = np.log((total_spam + alpha) / (total_spam + total_non_spam + 2 * alpha))\n",
    "    logPr_S0 = np.log((total_non_spam + alpha) / (total_spam + total_non_spam + 2 * alpha))\n",
    "    \n",
    "    logPr_W1_S1 = np.log((np.sum(X_train[y_train == 1], axis = 0) + alpha) / (total_spam + 2 * alpha))\n",
    "    logPr_W1_S0 = np.log((np.sum(X_train[y_train == 0], axis = 0) + alpha) / (total_non_spam + 2 * alpha))\n",
    "    \n",
    "    return logPr_S1, logPr_S0, logPr_W1_S1, logPr_W1_S0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e0eca-5bc6-4be6-b15e-fe90d5ffda03",
   "metadata": {
    "id": "854e0eca-5bc6-4be6-b15e-fe90d5ffda03"
   },
   "source": [
    "    2. (18pt) Add smoothing to the model. Smoothing amounts to assuming that we have “seen” every possible word α⩾0 times already, in both spam and non-spam emails. Note that αdoes not have to be an integer, and typically the best α<1.\n",
    "    What you have to do is to re-compute the probabilities log Pr(S), log Pr( ̄S), log Pr(w|S), log Pr(w| ̄S), the predictions part will remain unchanged. So you should update your fitting function by adding an additional argument αto it, and modify the probabilities accordingly. (And you use only training data for this.)\n",
    "    See Lecture Notes 7.3.2; Example 7.3 (page 274); and Schutt p 103 and p 109 for more explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d14e94-06fd-433b-bafb-03aff1ffe392",
   "metadata": {
    "id": "43d14e94-06fd-433b-bafb-03aff1ffe392",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a function for predicting\n",
    "def predicting(X_val, y_val):\n",
    "    L_S1 = logPr_S1 + X_val @ logPr_W1_S1.T\n",
    "    L_S0 = logPr_S0 + X_val @ logPr_W1_S0.T\n",
    "    \n",
    "    prediction = (L_S1 > L_S0) * 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93f900-4993-4870-b135-adf43ffe35ae",
   "metadata": {},
   "source": [
    "Fitting function is in 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940eb52-c846-4e69-aaaa-e9fe7962a795",
   "metadata": {
    "id": "c940eb52-c846-4e69-aaaa-e9fe7962a795"
   },
   "source": [
    "    3. (5pt) Use your updated model for predictions with a few different α-values (on validation data) and report the corresponding confusion matrix and accuracy.\n",
    "    A well-implemented algorith should not spend more than a few seconds on both fitting and predicting. However, more important that you understand what you are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b22e72-f267-413d-8c47-39143ece4afe",
   "metadata": {
    "id": "62b22e72-f267-413d-8c47-39143ece4afe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[422,  62],\n",
       "        [  0,  95]]),\n",
       " 0.8929188255613126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the updated model for predictions with alpha value of 1\n",
    "logPr_S1, logPr_S0, logPr_W1_S1, logPr_W1_S0 = fitting(X_train, y_train, 1) \n",
    "# Compute confusion matrix and accuracy\n",
    "cm = confusion_matrix(y_val, predicting(X_val, y_val))\n",
    "accuracy = accuracy_score(y_val, predicting(X_val, y_val))\n",
    "cm, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad9b44e-889e-4620-b4a0-3f5ac787749c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[459,  25],\n",
       "        [  0,  95]]),\n",
       " 0.9568221070811744)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the updated model for predictions with alpha value of 0.5\n",
    "logPr_S1, logPr_S0, logPr_W1_S1, logPr_W1_S0 = fitting(X_train, y_train, 0.5)\n",
    "# Compute confusion matrix and accuracy\n",
    "cm = confusion_matrix(y_val, predicting(X_val, y_val))\n",
    "accuracy = accuracy_score(y_val, predicting(X_val, y_val))\n",
    "cm, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b0fb8-1c82-491a-83cb-8ad9cd41f44c",
   "metadata": {
    "id": "e06b0fb8-1c82-491a-83cb-8ad9cd41f44c"
   },
   "source": [
    "    4. (5pt) Use validation to find the best smoothing parameter α.\n",
    "    You can just run a loop over different values, but start with very small values (10−8, 10−7, 10−6 up to perhaps 10).\n",
    "    Note: this is fairly fast if your algorithm is fast. But even if your algorithm is slow, do the best you can!\n",
    "    If your results are like mine, your best accuracy will be >99.5%. (But this result is random!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6367fad1-9afb-435c-a590-6f21a0dc62e3",
   "metadata": {
    "id": "6367fad1-9afb-435c-a590-6f21a0dc62e3"
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_alpha = 0\n",
    "scores = []\n",
    "alphas = []\n",
    "# Loop over different alpha values and find the best accuracy and best alpha value\n",
    "for n in range(-8, 2):\n",
    "    alpha = 10**n\n",
    "    logPr_S1, logPr_S0, logPr_W1_S1, logPr_W1_S0 = fitting(X_train, y_train, alpha)\n",
    "    score = accuracy_score(y_val, predicting(X_val, y_val))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "    scores.append(score)\n",
    "    alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fb3a54b-ff0d-4dc9-8abe-c1004f222657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is: 0.9948186528497409\n",
      "The best alpha value is: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('The best accuracy is:', best_score)\n",
    "print('The best alpha value is:', best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1de5dc-4674-40a4-91dd-a6a0d2f85ae4",
   "metadata": {
    "id": "8f1de5dc-4674-40a4-91dd-a6a0d2f85ae4"
   },
   "source": [
    "    5. (2pt) Plot how accuracy depends on α. Use log-scale for α!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c2f8461-a366-415a-99a8-e3bff4e732fe",
   "metadata": {
    "id": "8c2f8461-a366-415a-99a8-e3bff4e732fe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy vs. alpha using the log sclae for alpha\n",
    "_ = plt.plot(alphas, scores)\n",
    "_ = plt.xscale('log')\n",
    "_ = plt.ylim(0.9, 1)\n",
    "_ = plt.xticks(alphas)\n",
    "_ = plt.title(\"Alphas vs Accuracy\")\n",
    "_ = plt.xlabel(\"Alphas\")\n",
    "_ = plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e1612-cfee-4de7-bdd1-e9e73e6b7588",
   "metadata": {
    "id": "751e1612-cfee-4de7-bdd1-e9e73e6b7588"
   },
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646baf9-e840-48b0-9e66-aacafb60fa2e",
   "metadata": {
    "id": "8646baf9-e840-48b0-9e66-aacafb60fa2e"
   },
   "source": [
    "Naive Bayes is interpretable in a little similar fashion like linear regression. But in only a little similar fashion. Namely, we can find the words that are the best predictors that an email is spam, and the best predictors that email is non-spam. And we want to look at reasonably common words only, say more frequent than 10 times in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee1c3c-4e32-4e30-aae9-041acd130ba8",
   "metadata": {
    "id": "9bee1c3c-4e32-4e30-aae9-041acd130ba8"
   },
   "source": [
    "    1. (10pt) Which words are the best predictors that an email is spam? These are the word where Pr(S=1|W=1)is large and Pr(S=0|W=1)is small, or to put it differently, where log Pr(S=1|W=1)−log Pr(S=0|W=1)is large.\n",
    "    Explain why this is the case.\n",
    "    Hint: you may re-check the concept of log-likelihood and how that is used for prediction.\n",
    "    Hint 2: you may imagine you receive 60k 1-word emails (one for each word in your vocabulary). Which ones are most likely spam, and which ones are least likely spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a2ab204-c1f2-47bb-974d-b81860ea16b1",
   "metadata": {
    "id": "2a2ab204-c1f2-47bb-974d-b81860ea16b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words that are the best predictors for whether an email is a spam or not: ['bonus' 'profits' 'investment' 'fantastic' 'mlm']\n"
     ]
    }
   ],
   "source": [
    "# Fit with the best performing model\n",
    "logPr_S1, logPr_S0, logPr_W1_S1, logPr_W1_S0 = fitting(X_train, y_train, best_alpha)\n",
    "# Compute log Pr(S = 1|W = 1) and log Pr(S = 0|W = 1)\n",
    "logPr_S1_W1 = logPr_W1_S1 + logPr_S1\n",
    "logPr_S0_W1 = logPr_W1_S0 + logPr_S0\n",
    "# Compute the difference between the two log probabilities\n",
    "difference = logPr_S1_W1 - logPr_S0_W1\n",
    "difference = np.array(difference)\n",
    "# Convert vocabulary to array\n",
    "vocab = np.array(vocabulary)\n",
    "# Find the predictors \n",
    "predictors = vocab[np.argsort(-difference)]\n",
    "best_predictors = predictors[0, :5]\n",
    "print('The words that are the best predictors for whether an email is a spam or not:', best_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326ccc6-d77f-491e-8221-2a86e2b2cecb",
   "metadata": {},
   "source": [
    "To find the best predictors of whether an email is a spam or not, we want difference between $log(Pr(S = 1|W = 1))$ and $log(Pr(S = 0|W = 1))$ as large as possible. This is because it means that given a word, the email is significantly more likely to become a spam than non-spam. To calculate it, it is necessary to know $log(Pr(S = 1|W = 1))$ and $log(Pr(S = 0|W = 1))$\n",
    "\n",
    "To calculate the two necessary log probabilities, $log(Pr(S = 1|W = 1))$ could be rewrite as $log(Pr(W = 1|S = 1))* Pr(S = 1) / Pr(W = 1)$. Same logic, $log(Pr(S = 0|W = 1))$ can be rewrite as $log(Pr(W = 1|S = 0))* Pr(S = 0) / Pr(W = 1)$. Because both have the same denominator, we can ignore the denominator when doing the difference comparison. What we are really interested to find is the numerators on the top which is the difference between $log(Pr(W = 1|S = 1))* Pr(S = 1)$ and $log(Pr(W = 1|S = 0))* Pr(S = 0)$\n",
    "\n",
    "Finally, to compute the numerators, $log(Pr(W = 1|S = 1))* Pr(S = 1)$ can be rewritten as $log(Pr(W = 1|S = 1)) + log(Pr(S = 1))$, and $log(Pr(W = 1|S = 0))* Pr(S = 0)$ can be rewritten as $log(Pr(W = 1|S = 0)) + log(Pr(S = 0))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189dca4-f807-4a4e-8276-04554e6647ff",
   "metadata": {
    "id": "2189dca4-f807-4a4e-8276-04554e6647ff"
   },
   "source": [
    "    2. (10pt) Find 10 best words to predict spam and 10 best words to predict non-spam. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acc1d15d-8c86-4c2e-ba7e-2fa00ff244c1",
   "metadata": {
    "id": "acc1d15d-8c86-4c2e-ba7e-2fa00ff244c1"
   },
   "outputs": [],
   "source": [
    "# Find the indices of the 10 best words to predict spam and 10 best words to predict non-spam\n",
    "best_spams_predictors = np.argsort(-difference)[0, :10]\n",
    "best_non_spams_predictors = np.argsort(difference)[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b65f4c99-5a36-4942-aaed-c2f847b6eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 best words to predict spams are: ['bonus' 'profits' 'investment' 'fantastic' 'mlm' 'earning' 'stealth'\n",
      " 'relax' 'hottest' 'resell']\n"
     ]
    }
   ],
   "source": [
    "best_spam_words = vocab[best_spams_predictors]\n",
    "print('The 10 best words to predict spams are:', best_spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce679d53-205f-40ac-b7b4-3e6f9c8527e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 best words to predict non-spams are: ['linguistics' 'linguistic' 'theory' 'syntax' 'deadline' 'abstract'\n",
      " 'structure' 'grammar' 'committee' 'workshop']\n"
     ]
    }
   ],
   "source": [
    "best_non_spam_words = vocab[best_non_spams_predictors]\n",
    "print('The 10 best words to predict non-spams are:', best_non_spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46eb17-c1cc-4ebd-a6f3-ca881c1dab73",
   "metadata": {
    "id": "ae46eb17-c1cc-4ebd-a6f3-ca881c1dab73"
   },
   "source": [
    "# How many hours did you spend on this PS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1cdef-e0c0-4e17-9822-b6ca67e1835d",
   "metadata": {
    "id": "07668cc4-3934-4813-84f4-551a2c235920"
   },
   "source": [
    "I spent about 12 hours on this problem set."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "281d25b5-4193-4976-b5ae-80f5cecc07eb",
    "751e1612-cfee-4de7-bdd1-e9e73e6b7588",
    "ae46eb17-c1cc-4ebd-a6f3-ca881c1dab73",
    "fad9efe6-6ffc-482f-80c2-6d55da1cf756"
   ],
   "name": "Naive Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
